{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04246431-fbad-4b69-84f8-04cb3598ce4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pymongo\n",
    "import json\n",
    "\n",
    "# --- 1. Cargar el CSV de GitHub ---\n",
    "try:\n",
    "    df_github = pd.read_csv('lenguajesGithub.csv')\n",
    "    print(f\"Dataset de GitHub cargado con {len(df_github)} filas.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: Revisa el nombre del archivo CSV de GitHub.\")\n",
    "    \n",
    "\n",
    "# --- 2. Limpieza y Ajuste de Tamaño ---\n",
    "# Por ahora, nos aseguramos de no tener duplicados.\n",
    "df_github.drop_duplicates(inplace=True)\n",
    "\n",
    "# Ajustamos al tamaño final de 90,000 registros\n",
    "tamano_objetivo_github = 90000\n",
    "if len(df_github) > tamano_objetivo_github:\n",
    "    df_github_final = df_github.sample(n=tamano_objetivo_github, random_state=42)\n",
    "else:\n",
    "    df_github_final = df_github\n",
    "print(f\"DataFrame ajustado a {len(df_github_final)} filas.\")\n",
    "\n",
    "\n",
    "# --- 3. Evidenciar la Conversión (DataFrame -> JSON) ---\n",
    "# Guardamos el DataFrame limpio como un archivo JSON\n",
    "df_github_final.to_json('github_data_limpio.json', orient='records', indent=4)\n",
    "print(\"Datos guardados como 'github_data_limpio.json'.\")\n",
    "\n",
    "\n",
    "# --- 4. Cargar a MongoDB Atlas ---\n",
    "# Reemplaza <tu_contraseña> con tu contraseña real\n",
    "MONGO_URI = \"mongodb+srv://juan:<tu_contraseña>@cluster0.agcdm8h.mongodb.net/\"\n",
    "\n",
    "try:\n",
    "    client = pymongo.MongoClient(MONGO_URI)\n",
    "    client.admin.command('ping')\n",
    "    print(\"¡Conexión a MongoDB Atlas exitosa!\")\n",
    "\n",
    "    db = client['proyecto_futbol'] # Usamos la misma base de datos\n",
    "    collection = db['repositorios_github'] # ¡PERO UNA NUEVA COLECCIÓN!\n",
    "\n",
    "    # Convertimos el DataFrame a un formato compatible con Mongo\n",
    "    datos_para_mongo = df_github_final.to_dict(orient='records')\n",
    "    \n",
    "    print(f\"Limpiando la colección '{collection.name}'...\")\n",
    "    collection.delete_many({})\n",
    "\n",
    "    # Insertamos los datos en lotes (la forma segura que ya conocemos)\n",
    "    batch_size = 10000\n",
    "    print(f\"Insertando {len(datos_para_mongo)} documentos en la nueva colección...\")\n",
    "    for i in range(0, len(datos_para_mongo), batch_size):\n",
    "        batch = datos_para_mongo[i:i + batch_size]\n",
    "        collection.insert_many(batch)\n",
    "        print(f\"  -> Lote {i//batch_size + 1} insertado.\")\n",
    "\n",
    "    print(\"¡Carga de datos de GitHub completada!\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error durante el proceso de carga a MongoDB: {e}\")\n",
    "\n",
    "finally:\n",
    "    if 'client' in locals():\n",
    "        client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c1fd3f6-7051-403e-a1d9-de3930044878",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset de GitHub cargado con 90000 filas.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pymongo\n",
    "import json\n",
    "\n",
    "# --- 1. Cargar el CSV de GitHub ---\n",
    "try:\n",
    "    df_github = pd.read_csv('lenguajesGithub.csv')\n",
    "    print(f\"Dataset de GitHub cargado con {len(df_github)} filas.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: Revisa el nombre del archivo CSV de GitHub.\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "79b7b68c-8f42-4555-9917-9dc8e00d4e85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame ajustado a 90000 filas.\n"
     ]
    }
   ],
   "source": [
    "# --- 2. Limpieza y Ajuste de Tamaño ---\n",
    "# Por ahora, nos aseguramos de no tener duplicados.\n",
    "df_github.drop_duplicates(inplace=True)\n",
    "\n",
    "# Ajustamos al tamaño final de 90,000 registros\n",
    "tamano_objetivo_github = 90000\n",
    "if len(df_github) > tamano_objetivo_github:\n",
    "    df_github_final = df_github.sample(n=tamano_objetivo_github, random_state=42)\n",
    "else:\n",
    "    df_github_final = df_github\n",
    "print(f\"DataFrame ajustado a {len(df_github_final)} filas.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ebac747-6b57-4f19-bd07-909dbdd17759",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datos guardados como 'github_data_limpio.json'.\n"
     ]
    }
   ],
   "source": [
    "# --- 3. Evidenciar la Conversión  ---\n",
    "# Guardamos el DataFrame limpio como un archivo JSON\n",
    "df_github_final.to_json('github_data_limpio.json', orient='records', indent=4)\n",
    "print(\"Datos guardados como 'github_data_limpio.json'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff5d0218-ab8d-4c96-bcac-8bbd0f7d5fd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¡Conexión a MongoDB Atlas exitosa!\n",
      "Limpiando la colección 'repositorios_github'...\n",
      "Insertando 90000 documentos en la nueva colección...\n",
      "  -> Lote 1 insertado.\n",
      "  -> Lote 2 insertado.\n",
      "  -> Lote 3 insertado.\n",
      "  -> Lote 4 insertado.\n",
      "  -> Lote 5 insertado.\n",
      "  -> Lote 6 insertado.\n",
      "  -> Lote 7 insertado.\n",
      "  -> Lote 8 insertado.\n",
      "  -> Lote 9 insertado.\n",
      "¡Carga de datos de GitHub completada!\n"
     ]
    }
   ],
   "source": [
    "# --- 4. Cargar a MongoDB Atlas ---\n",
    "\n",
    "MONGO_URI = \"mongodb+srv://juan:12345@cluster0.agcdm8h.mongodb.net/\"\n",
    "\n",
    "try:\n",
    "    client = pymongo.MongoClient(MONGO_URI)\n",
    "    client.admin.command('ping')\n",
    "    print(\"¡Conexión a MongoDB Atlas exitosa!\")\n",
    "\n",
    "    db = client['proyecto_futbol'] \n",
    "    collection = db['repositorios_github'] # creacmos una nueva coleccion\n",
    "\n",
    "    # Convertimos el DataFrame a un formato compatible con Mongo\n",
    "    datos_para_mongo = df_github_final.to_dict(orient='records')\n",
    "    \n",
    "    print(f\"Limpiando la colección '{collection.name}'...\")\n",
    "    collection.delete_many({})\n",
    "\n",
    "    # Insertamos los datos en lotes \n",
    "    batch_size = 10000\n",
    "    print(f\"Insertando {len(datos_para_mongo)} documentos en la nueva colección...\")\n",
    "    for i in range(0, len(datos_para_mongo), batch_size):\n",
    "        batch = datos_para_mongo[i:i + batch_size]\n",
    "        collection.insert_many(batch)\n",
    "        print(f\"  -> Lote {i//batch_size + 1} insertado.\")\n",
    "\n",
    "    print(\"¡Carga de datos de GitHub completada!\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error durante el proceso de carga a MongoDB: {e}\")\n",
    "\n",
    "finally:\n",
    "    if 'client' in locals():\n",
    "        client.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
